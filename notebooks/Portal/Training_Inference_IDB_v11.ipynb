{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with models trained on Baranquilla 40cm imagery and assessed on Baranquilla Resampled 40cm test imagery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how you can launch training script, compare model performance and load the weights of a trained model, then use this for inference and store the segmentation maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File /home/root/logs not found!\n",
      "File /home/root/data/local/raw not found!\n",
      "File /home/root/data/local/intermediate not found!\n",
      "File /home/root/data/volumes/ebs_inference_storage not found!\n",
      "File /home/root/data/volumes/datasets/processed not found!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- timbermafia has configured handlers:\n",
      "  - <StreamHandler stdout (NOTSET)>\n",
      "- name gim-cv given, generated named logger.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Trains a Segmentalist model on a dataset and saves the resulting model checkpoints.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import re\n",
    "import operator\n",
    "import logging\n",
    "import pickle\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask\n",
    "import dask.array as da\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import kerastuner as kt\n",
    "import sys\n",
    "import pprint\n",
    "import yaml\n",
    "import uuid\n",
    "import albumentations as A\n",
    "import joblib \n",
    "\n",
    "import gim_cv.config as cfg\n",
    "import gim_cv.utils as utils\n",
    "import gim_cv.losses as losses\n",
    "import gim_cv.datasets as datasets\n",
    "import gim_cv.tools.keras_one_cycle_clr as clr\n",
    "\n",
    "from functools import partial, reduce\n",
    "from pathlib import Path\n",
    "from time import perf_counter as pc\n",
    "\n",
    "#from sklearn.externals import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from distributed import Client, LocalCluster\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow_addons.optimizers import SWA\n",
    "#from zarr.errors import ArrayNotFoundError\n",
    "\n",
    "from gim_cv.models.segmentalist import Segmentalist\n",
    "from gim_cv.training import TrainingDataset, pair_batch_generator, CompositeTrainingDataset, fancy_batch_generator\n",
    "from gim_cv.datasets import (get_dataset,\n",
    "                             get_image_training_pipeline_by_tag,\n",
    "                             get_binary_mask_training_pipeline_by_tag,\n",
    "                             list_datasets)\n",
    "from gim_cv.preprocessing import get_aug_datagen, FancyPCA, strong_aug, balanced_oversample\n",
    "from gim_cv.utils import plot_pair, parse_kwarg_str\n",
    "from gim_cv.tuners import HyperbandOCP\n",
    "###\n",
    "import os\n",
    "import time\n",
    "\n",
    "# the model saving/loading utility functions live in the utils.py module in bin with the scripts\n",
    "sys.path.append('../../bin')\n",
    "from bin import utils\n",
    "from PIL import Image\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "import math\n",
    "import imageio\n",
    "import numbers\n",
    "import rioxarray as rx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;255m21:36:44  I  …iteTrainingDataset.prune  \u001b[0m\u001b[38;5;255m\u001b[38;5;231mSelecting training datasets to eliminate with\u001b[0m\u001b[38;5;255m               \u001b[0m\n",
      "\u001b[38;5;255m                                        \u001b[0m\u001b[38;5;255m\u001b[38;5;231mhas_empty_raster...\u001b[0m\u001b[38;5;255m                                         \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D  …iningDataset.load_arrays  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mloading arrays\u001b[0m\u001b[38;5;33m                                              \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D  …iningDataset.load_arrays  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mdone loading arrays\u001b[0m\u001b[38;5;33m                                         \u001b[0m\n",
      "\u001b[38;5;255m21:36:44  I  …iteTrainingDataset.prune  \u001b[0m\u001b[38;5;255m\u001b[38;5;231mRemoved 0 TrainingDatasets, leaving 1.\u001b[0m\u001b[38;5;255m                      \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D    TrainingDataset.prepare  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mObtaining arrays for\u001b[0m\u001b[38;5;33m                                        \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231m/home/root/tests/resources/test_data_tif/vienna1_1_1.tif, /h\u001b[0m\u001b[38;5;33m\u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mome/root/tests/resources/test_data_tif/vienna1_mask_1_1.tif.\u001b[0m\u001b[38;5;33m\u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231m..\u001b[0m\u001b[38;5;33m                                                          \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D  …iningDataset.load_arrays  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mloading arrays\u001b[0m\u001b[38;5;33m                                              \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D  …iningDataset.load_arrays  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mdone loading arrays\u001b[0m\u001b[38;5;33m                                         \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D  …ngDataset.make_pipelines  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mAssigning image pipeline...\u001b[0m\u001b[38;5;33m                                 \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D  …ngDataset.make_pipelines  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mAssigning mask pipeline...\u001b[0m\u001b[38;5;33m                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D   ImageResampler.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mCasting resampled array back to 8-bit integers...\u001b[0m\u001b[38;5;33m           \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D       OverlappingTiler.fit  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mDividing array with shape (781, 781, 3) into half-step\u001b[0m\u001b[38;5;33m      \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231moverlapping tiles...\u001b[0m\u001b[38;5;33m                                        \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D                  Tiler.fit  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mShape going into stacker: (781, 781, 3)\u001b[0m\u001b[38;5;33m                     \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D                  Tiler.fit  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mShape going into stacker: (653, 781, 3)\u001b[0m\u001b[38;5;33m                     \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D                  Tiler.fit  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mShape going into stacker: (781, 653, 3)\u001b[0m\u001b[38;5;33m                     \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D                  Tiler.fit  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mShape going into stacker: (653, 653, 3)\u001b[0m\u001b[38;5;33m                     \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:44  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;33m21:36:45  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D  …erlappingTiler.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mAssembling overlapping tiles...\u001b[0m\u001b[38;5;33m                             \u001b[0m\n",
      "\u001b[38;5;255m21:36:45  I    TrainingDataset.prepare  \u001b[0m\u001b[38;5;255m\u001b[38;5;231mImage pipeline done!\u001b[0m\u001b[38;5;255m                                        \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D   DimensionAdder.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mLeaving array shape alone...\u001b[0m\u001b[38;5;33m                                \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D       OverlappingTiler.fit  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mDividing array with shape (781, 781, 1) into half-step\u001b[0m\u001b[38;5;33m      \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231moverlapping tiles...\u001b[0m\u001b[38;5;33m                                        \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D                  Tiler.fit  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mShape going into stacker: (781, 781, 1)\u001b[0m\u001b[38;5;33m                     \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D                  Tiler.fit  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mShape going into stacker: (653, 781, 1)\u001b[0m\u001b[38;5;33m                     \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D                  Tiler.fit  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mShape going into stacker: (781, 653, 1)\u001b[0m\u001b[38;5;33m                     \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D                  Tiler.fit  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mShape going into stacker: (653, 653, 1)\u001b[0m\u001b[38;5;33m                     \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D     WindowFitter.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mTransforming input data to match integer number of\u001b[0m\u001b[38;5;33m          \u001b[0m\n",
      "\u001b[38;5;33m                                        \u001b[0m\u001b[38;5;33m\u001b[38;5;231mwindows...\u001b[0m\u001b[38;5;33m                                                  \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D      TileStacker.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mReshaping array into tiles...\u001b[0m\u001b[38;5;33m                               \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D  …erlappingTiler.transform  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mAssembling overlapping tiles...\u001b[0m\u001b[38;5;33m                             \u001b[0m\n",
      "\u001b[38;5;255m21:36:45  I    TrainingDataset.prepare  \u001b[0m\u001b[38;5;255m\u001b[38;5;231mMask pipeline done!\u001b[0m\u001b[38;5;255m                                         \u001b[0m\n",
      "\u001b[38;5;33m21:36:45  D    TrainingDataset.prepare  \u001b[0m\u001b[38;5;33m\u001b[38;5;231mAligning image/mask array chunks...\u001b[0m\u001b[38;5;33m                         \u001b[0m\n",
      "\u001b[38;5;255m21:36:45  I    TrainingDataset.prepare  \u001b[0m\u001b[38;5;255m\u001b[38;5;231mImage/mask arrays from\u001b[0m\u001b[38;5;255m                                      \u001b[0m\n",
      "\u001b[38;5;255m                                        \u001b[0m\u001b[38;5;255m\u001b[38;5;231m/home/root/tests/resources/test_data_tif/vienna1_1_1.tif <->\u001b[0m\u001b[38;5;255m\u001b[0m\n",
      "\u001b[38;5;255m                                        \u001b[0m\u001b[38;5;255m\u001b[38;5;231m/home/root/tests/resources/test_data_tif/vienna1_mask_1_1.ti\u001b[0m\u001b[38;5;255m\u001b[0m\n",
      "\u001b[38;5;255m                                        \u001b[0m\u001b[38;5;255m\u001b[38;5;231mf prepared for training!\u001b[0m\u001b[38;5;255m                                    \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;255m21:36:45  I  …eTrainingDataset.prepare  \u001b[0m\u001b[38;5;255m\u001b[38;5;231mAligning image/mask array chunks...\u001b[0m\u001b[38;5;255m                         \u001b[0m\n",
      "\u001b[38;5;255m21:36:45  I  …eTrainingDataset.prepare  \u001b[0m\u001b[38;5;255m\u001b[38;5;231mShuffling datasets together...\u001b[0m\u001b[38;5;255m                              \u001b[0m\n",
      "\u001b[38;5;255m21:36:45  I  …eTrainingDataset.prepare  \u001b[0m\u001b[38;5;255m\u001b[38;5;231mComposite dataset prepare done!\u001b[0m\u001b[38;5;255m                             \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "~~ Fancy augs on ~~\n",
      "/opt/conda/envs/gim_cv_gpu/lib/python3.8/site-packages/distributed/node.py:151: UserWarning:\n",
      "\n",
      "Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 34365 instead\n",
      "\n",
      "Eigenvalues are [0.1163573  0.00178379 0.00056363]\n",
      "Eigenvectors are [[-0.62135921 -0.73644309  0.26751507]\n",
      " [-0.54756572  0.16393143 -0.82054754]\n",
      " [-0.56043244  0.65633685  0.50511129]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps=11\n",
      "val_steps=0\n",
      "fit model\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 12 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 14 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 12 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 12 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 12 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 13 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 29 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 29 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 30 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 16 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 29 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 30 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 16 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function SWA.average_op at 0x7f34ac72dc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      " 2/11 [====>.........................] - ETA: 36s - loss: 0.6080 - tversky_index: 0.4980 - jaccard_index: 0.2439 - recall: 0.6827 - precision: 0.2750 - specificity: 0.4578 - npv: 0.8268 - dice_coefficient: 0.3920WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (2.641256). Check your callbacks.\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5614 - tversky_index: 0.5542 - jaccard_index: 0.2816 - recall: 0.7550 - precision: 0.3099 - specificity: 0.5771 - npv: 0.9011 - dice_coefficient: 0.4386\n",
      "Epoch 00001: loss improved from inf to 0.56143, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e01-ji0.28159-l0.56143-vl0.00.ckpt\n",
      "11/11 [==============================] - 34s 3s/step - loss: 0.5614 - tversky_index: 0.5542 - jaccard_index: 0.2816 - recall: 0.7550 - precision: 0.3099 - specificity: 0.5771 - npv: 0.9011 - dice_coefficient: 0.4386 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4923 - tversky_index: 0.5927 - jaccard_index: 0.3408 - recall: 0.7131 - precision: 0.3951 - specificity: 0.7297 - npv: 0.9117 - dice_coefficient: 0.5077\n",
      "Epoch 00002: loss improved from 0.56143 to 0.49231, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e02-ji0.34078-l0.49231-vl0.00.ckpt\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.4923 - tversky_index: 0.5927 - jaccard_index: 0.3408 - recall: 0.7131 - precision: 0.3951 - specificity: 0.7297 - npv: 0.9117 - dice_coefficient: 0.5077 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4210 - tversky_index: 0.6337 - jaccard_index: 0.4087 - recall: 0.7014 - precision: 0.4955 - specificity: 0.8243 - npv: 0.9194 - dice_coefficient: 0.5790\n",
      "Epoch 00003: loss improved from 0.49231 to 0.42100, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e03-ji0.40873-l0.42100-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.4210 - tversky_index: 0.6337 - jaccard_index: 0.4087 - recall: 0.7014 - precision: 0.4955 - specificity: 0.8243 - npv: 0.9194 - dice_coefficient: 0.5790 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4151 - tversky_index: 0.6041 - jaccard_index: 0.4157 - recall: 0.6257 - precision: 0.5522 - specificity: 0.8748 - npv: 0.9058 - dice_coefficient: 0.5849\n",
      "Epoch 00004: loss improved from 0.42100 to 0.41513, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e04-ji0.41567-l0.41513-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.4151 - tversky_index: 0.6041 - jaccard_index: 0.4157 - recall: 0.6257 - precision: 0.5522 - specificity: 0.8748 - npv: 0.9058 - dice_coefficient: 0.5849 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.3568 - tversky_index: 0.6708 - jaccard_index: 0.4757 - recall: 0.7023 - precision: 0.5962 - specificity: 0.8857 - npv: 0.9254 - dice_coefficient: 0.6432\n",
      "Epoch 00005: loss improved from 0.41513 to 0.35682, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e05-ji0.47575-l0.35682-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.3568 - tversky_index: 0.6708 - jaccard_index: 0.4757 - recall: 0.7023 - precision: 0.5962 - specificity: 0.8857 - npv: 0.9254 - dice_coefficient: 0.6432 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.3258 - tversky_index: 0.6845 - jaccard_index: 0.5095 - recall: 0.6962 - precision: 0.6570 - specificity: 0.9125 - npv: 0.9248 - dice_coefficient: 0.6742\n",
      "Epoch 00006: loss improved from 0.35682 to 0.32579, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e06-ji0.50949-l0.32579-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.3258 - tversky_index: 0.6845 - jaccard_index: 0.5095 - recall: 0.6962 - precision: 0.6570 - specificity: 0.9125 - npv: 0.9248 - dice_coefficient: 0.6742 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.3012 - tversky_index: 0.7103 - jaccard_index: 0.5384 - recall: 0.7232 - precision: 0.6794 - specificity: 0.9161 - npv: 0.9308 - dice_coefficient: 0.6988\n",
      "Epoch 00007: loss improved from 0.32579 to 0.30124, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e07-ji0.53840-l0.30124-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.3012 - tversky_index: 0.7103 - jaccard_index: 0.5384 - recall: 0.7232 - precision: 0.6794 - specificity: 0.9161 - npv: 0.9308 - dice_coefficient: 0.6988 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.3124 - tversky_index: 0.6920 - jaccard_index: 0.5263 - recall: 0.6968 - precision: 0.6803 - specificity: 0.9196 - npv: 0.9252 - dice_coefficient: 0.6876\n",
      "Epoch 00008: loss did not improve from 0.30124\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.3124 - tversky_index: 0.6920 - jaccard_index: 0.5263 - recall: 0.6968 - precision: 0.6803 - specificity: 0.9196 - npv: 0.9252 - dice_coefficient: 0.6876 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.2663 - tversky_index: 0.7385 - jaccard_index: 0.5810 - recall: 0.7439 - precision: 0.7259 - specificity: 0.9316 - npv: 0.9364 - dice_coefficient: 0.7337\n",
      "Epoch 00009: loss improved from 0.30124 to 0.26626, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e09-ji0.58103-l0.26626-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.2663 - tversky_index: 0.7385 - jaccard_index: 0.5810 - recall: 0.7439 - precision: 0.7259 - specificity: 0.9316 - npv: 0.9364 - dice_coefficient: 0.7337 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.2537 - tversky_index: 0.7477 - jaccard_index: 0.5961 - recall: 0.7493 - precision: 0.7441 - specificity: 0.9372 - npv: 0.9387 - dice_coefficient: 0.7463\n",
      "Epoch 00010: loss improved from 0.26626 to 0.25367, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e10-ji0.59613-l0.25367-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.2537 - tversky_index: 0.7477 - jaccard_index: 0.5961 - recall: 0.7493 - precision: 0.7441 - specificity: 0.9372 - npv: 0.9387 - dice_coefficient: 0.7463 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.2637 - tversky_index: 0.7286 - jaccard_index: 0.5833 - recall: 0.7214 - precision: 0.7533 - specificity: 0.9436 - npv: 0.9343 - dice_coefficient: 0.7363\n",
      "Epoch 00011: loss did not improve from 0.25367\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.2637 - tversky_index: 0.7286 - jaccard_index: 0.5833 - recall: 0.7214 - precision: 0.7533 - specificity: 0.9436 - npv: 0.9343 - dice_coefficient: 0.7363 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.2553 - tversky_index: 0.7514 - jaccard_index: 0.5942 - recall: 0.7585 - precision: 0.7322 - specificity: 0.9329 - npv: 0.9409 - dice_coefficient: 0.7447\n",
      "Epoch 00012: loss did not improve from 0.25367\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.2553 - tversky_index: 0.7514 - jaccard_index: 0.5942 - recall: 0.7585 - precision: 0.7322 - specificity: 0.9329 - npv: 0.9409 - dice_coefficient: 0.7447 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.2472 - tversky_index: 0.7519 - jaccard_index: 0.6049 - recall: 0.7513 - precision: 0.7558 - specificity: 0.9406 - npv: 0.9391 - dice_coefficient: 0.7528\n",
      "Epoch 00013: loss improved from 0.25367 to 0.24718, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e13-ji0.60495-l0.24718-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.2472 - tversky_index: 0.7519 - jaccard_index: 0.6049 - recall: 0.7513 - precision: 0.7558 - specificity: 0.9406 - npv: 0.9391 - dice_coefficient: 0.7528 - lr: 5.0000e-05\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.2364 - tversky_index: 0.7635 - jaccard_index: 0.6195 - recall: 0.7639 - precision: 0.7647 - specificity: 0.9425 - npv: 0.9420 - dice_coefficient: 0.7636\n",
      "Epoch 00014: loss improved from 0.24718 to 0.23644, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e14-ji0.61953-l0.23644-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.2364 - tversky_index: 0.7635 - jaccard_index: 0.6195 - recall: 0.7639 - precision: 0.7647 - specificity: 0.9425 - npv: 0.9420 - dice_coefficient: 0.7636 - lr: 5.0000e-05\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.2410 - tversky_index: 0.7567 - jaccard_index: 0.6123 - recall: 0.7547 - precision: 0.7648 - specificity: 0.9441 - npv: 0.9403 - dice_coefficient: 0.7590\n",
      "Epoch 00015: loss did not improve from 0.23644\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.2410 - tversky_index: 0.7567 - jaccard_index: 0.6123 - recall: 0.7547 - precision: 0.7648 - specificity: 0.9441 - npv: 0.9403 - dice_coefficient: 0.7590 - lr: 5.0000e-05\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.2259 - tversky_index: 0.7723 - jaccard_index: 0.6323 - recall: 0.7710 - precision: 0.7789 - specificity: 0.9471 - npv: 0.9438 - dice_coefficient: 0.7741\n",
      "Epoch 00016: loss improved from 0.23644 to 0.22589, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e16-ji0.63233-l0.22589-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.2259 - tversky_index: 0.7723 - jaccard_index: 0.6323 - recall: 0.7710 - precision: 0.7789 - specificity: 0.9471 - npv: 0.9438 - dice_coefficient: 0.7741 - lr: 5.0000e-05\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.2227 - tversky_index: 0.7755 - jaccard_index: 0.6373 - recall: 0.7742 - precision: 0.7818 - specificity: 0.9477 - npv: 0.9449 - dice_coefficient: 0.7773\n",
      "Epoch 00017: loss improved from 0.22589 to 0.22275, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e17-ji0.63727-l0.22275-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.2227 - tversky_index: 0.7755 - jaccard_index: 0.6373 - recall: 0.7742 - precision: 0.7818 - specificity: 0.9477 - npv: 0.9449 - dice_coefficient: 0.7773 - lr: 5.0000e-05\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.2209 - tversky_index: 0.7719 - jaccard_index: 0.6390 - recall: 0.7651 - precision: 0.7950 - specificity: 0.9528 - npv: 0.9432 - dice_coefficient: 0.7791\n",
      "Epoch 00018: loss improved from 0.22275 to 0.22087, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e18-ji0.63904-l0.22087-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.2209 - tversky_index: 0.7719 - jaccard_index: 0.6390 - recall: 0.7651 - precision: 0.7950 - specificity: 0.9528 - npv: 0.9432 - dice_coefficient: 0.7791 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.2082 - tversky_index: 0.7915 - jaccard_index: 0.6564 - recall: 0.7915 - precision: 0.7931 - specificity: 0.9501 - npv: 0.9491 - dice_coefficient: 0.7918\n",
      "Epoch 00019: loss improved from 0.22087 to 0.20821, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e19-ji0.65641-l0.20821-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.2082 - tversky_index: 0.7915 - jaccard_index: 0.6564 - recall: 0.7915 - precision: 0.7931 - specificity: 0.9501 - npv: 0.9491 - dice_coefficient: 0.7918 - lr: 5.0000e-05\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.2029 - tversky_index: 0.7894 - jaccard_index: 0.6633 - recall: 0.7821 - precision: 0.8135 - specificity: 0.9564 - npv: 0.9469 - dice_coefficient: 0.7971\n",
      "Epoch 00020: loss improved from 0.20821 to 0.20290, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e20-ji0.66331-l0.20290-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.2029 - tversky_index: 0.7894 - jaccard_index: 0.6633 - recall: 0.7821 - precision: 0.8135 - specificity: 0.9564 - npv: 0.9469 - dice_coefficient: 0.7971 - lr: 5.0000e-05\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.2034 - tversky_index: 0.7960 - jaccard_index: 0.6638 - recall: 0.7958 - precision: 0.7989 - specificity: 0.9513 - npv: 0.9497 - dice_coefficient: 0.7966\n",
      "Epoch 00021: loss did not improve from 0.20290\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.2034 - tversky_index: 0.7960 - jaccard_index: 0.6638 - recall: 0.7958 - precision: 0.7989 - specificity: 0.9513 - npv: 0.9497 - dice_coefficient: 0.7966 - lr: 5.0000e-05\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.2024 - tversky_index: 0.7903 - jaccard_index: 0.6639 - recall: 0.7833 - precision: 0.8129 - specificity: 0.9561 - npv: 0.9471 - dice_coefficient: 0.7976\n",
      "Epoch 00022: loss improved from 0.20290 to 0.20240, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e22-ji0.66391-l0.20240-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.2024 - tversky_index: 0.7903 - jaccard_index: 0.6639 - recall: 0.7833 - precision: 0.8129 - specificity: 0.9561 - npv: 0.9471 - dice_coefficient: 0.7976 - lr: 5.0000e-05\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.2010 - tversky_index: 0.7938 - jaccard_index: 0.6658 - recall: 0.7887 - precision: 0.8102 - specificity: 0.9550 - npv: 0.9484 - dice_coefficient: 0.7990\n",
      "Epoch 00023: loss improved from 0.20240 to 0.20100, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e23-ji0.66585-l0.20100-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.2010 - tversky_index: 0.7938 - jaccard_index: 0.6658 - recall: 0.7887 - precision: 0.8102 - specificity: 0.9550 - npv: 0.9484 - dice_coefficient: 0.7990 - lr: 5.0000e-05\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1894 - tversky_index: 0.8045 - jaccard_index: 0.6827 - recall: 0.7985 - precision: 0.8234 - specificity: 0.9576 - npv: 0.9500 - dice_coefficient: 0.8106\n",
      "Epoch 00024: loss improved from 0.20100 to 0.18942, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e24-ji0.68268-l0.18942-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.1894 - tversky_index: 0.8045 - jaccard_index: 0.6827 - recall: 0.7985 - precision: 0.8234 - specificity: 0.9576 - npv: 0.9500 - dice_coefficient: 0.8106 - lr: 5.0000e-05\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.2019 - tversky_index: 0.7985 - jaccard_index: 0.6648 - recall: 0.7991 - precision: 0.7978 - specificity: 0.9519 - npv: 0.9516 - dice_coefficient: 0.7981\n",
      "Epoch 00025: loss did not improve from 0.18942\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.2019 - tversky_index: 0.7985 - jaccard_index: 0.6648 - recall: 0.7991 - precision: 0.7978 - specificity: 0.9519 - npv: 0.9516 - dice_coefficient: 0.7981 - lr: 5.0000e-05\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.2152 - tversky_index: 0.7757 - jaccard_index: 0.6476 - recall: 0.7672 - precision: 0.8045 - specificity: 0.9548 - npv: 0.9438 - dice_coefficient: 0.7848\n",
      "Epoch 00026: loss did not improve from 0.18942\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.2152 - tversky_index: 0.7757 - jaccard_index: 0.6476 - recall: 0.7672 - precision: 0.8045 - specificity: 0.9548 - npv: 0.9438 - dice_coefficient: 0.7848 - lr: 5.0000e-05\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1890 - tversky_index: 0.8099 - jaccard_index: 0.6829 - recall: 0.8091 - precision: 0.8143 - specificity: 0.9546 - npv: 0.9527 - dice_coefficient: 0.8110\n",
      "Epoch 00027: loss improved from 0.18942 to 0.18899, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e27-ji0.68295-l0.18899-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.1890 - tversky_index: 0.8099 - jaccard_index: 0.6829 - recall: 0.8091 - precision: 0.8143 - specificity: 0.9546 - npv: 0.9527 - dice_coefficient: 0.8110 - lr: 2.5000e-05\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1876 - tversky_index: 0.8129 - jaccard_index: 0.6853 - recall: 0.8135 - precision: 0.8119 - specificity: 0.9546 - npv: 0.9546 - dice_coefficient: 0.8124\n",
      "Epoch 00028: loss improved from 0.18899 to 0.18760, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e28-ji0.68530-l0.18760-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.1876 - tversky_index: 0.8129 - jaccard_index: 0.6853 - recall: 0.8135 - precision: 0.8119 - specificity: 0.9546 - npv: 0.9546 - dice_coefficient: 0.8124 - lr: 2.5000e-05\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1835 - tversky_index: 0.8109 - jaccard_index: 0.6904 - recall: 0.8055 - precision: 0.8282 - specificity: 0.9598 - npv: 0.9531 - dice_coefficient: 0.8165\n",
      "Epoch 00029: loss improved from 0.18760 to 0.18353, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e29-ji0.69039-l0.18353-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.1835 - tversky_index: 0.8109 - jaccard_index: 0.6904 - recall: 0.8055 - precision: 0.8282 - specificity: 0.9598 - npv: 0.9531 - dice_coefficient: 0.8165 - lr: 2.5000e-05\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1923 - tversky_index: 0.8018 - jaccard_index: 0.6786 - recall: 0.7962 - precision: 0.8203 - specificity: 0.9578 - npv: 0.9504 - dice_coefficient: 0.8077\n",
      "Epoch 00030: loss did not improve from 0.18353\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.1923 - tversky_index: 0.8018 - jaccard_index: 0.6786 - recall: 0.7962 - precision: 0.8203 - specificity: 0.9578 - npv: 0.9504 - dice_coefficient: 0.8077 - lr: 2.5000e-05\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1988 - tversky_index: 0.7976 - jaccard_index: 0.6696 - recall: 0.7943 - precision: 0.8096 - specificity: 0.9549 - npv: 0.9501 - dice_coefficient: 0.8012\n",
      "Epoch 00031: loss did not improve from 0.18353\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.1988 - tversky_index: 0.7976 - jaccard_index: 0.6696 - recall: 0.7943 - precision: 0.8096 - specificity: 0.9549 - npv: 0.9501 - dice_coefficient: 0.8012 - lr: 2.5000e-05\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1994 - tversky_index: 0.7977 - jaccard_index: 0.6693 - recall: 0.7950 - precision: 0.8073 - specificity: 0.9538 - npv: 0.9497 - dice_coefficient: 0.8006\n",
      "Epoch 00032: loss did not improve from 0.18353\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.1994 - tversky_index: 0.7977 - jaccard_index: 0.6693 - recall: 0.7950 - precision: 0.8073 - specificity: 0.9538 - npv: 0.9497 - dice_coefficient: 0.8006 - lr: 1.2500e-05\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1735 - tversky_index: 0.8214 - jaccard_index: 0.7048 - recall: 0.8165 - precision: 0.8372 - specificity: 0.9612 - npv: 0.9550 - dice_coefficient: 0.8265\n",
      "Epoch 00033: loss improved from 0.18353 to 0.17354, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e33-ji0.70477-l0.17354-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.1735 - tversky_index: 0.8214 - jaccard_index: 0.7048 - recall: 0.8165 - precision: 0.8372 - specificity: 0.9612 - npv: 0.9550 - dice_coefficient: 0.8265 - lr: 1.2500e-05\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1796 - tversky_index: 0.8145 - jaccard_index: 0.6964 - recall: 0.8089 - precision: 0.8328 - specificity: 0.9609 - npv: 0.9538 - dice_coefficient: 0.8204\n",
      "Epoch 00034: loss did not improve from 0.17354\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.1796 - tversky_index: 0.8145 - jaccard_index: 0.6964 - recall: 0.8089 - precision: 0.8328 - specificity: 0.9609 - npv: 0.9538 - dice_coefficient: 0.8204 - lr: 1.2500e-05\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1860 - tversky_index: 0.8076 - jaccard_index: 0.6875 - recall: 0.8016 - precision: 0.8276 - specificity: 0.9594 - npv: 0.9514 - dice_coefficient: 0.8140\n",
      "Epoch 00035: loss did not improve from 0.17354\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.1860 - tversky_index: 0.8076 - jaccard_index: 0.6875 - recall: 0.8016 - precision: 0.8276 - specificity: 0.9594 - npv: 0.9514 - dice_coefficient: 0.8140 - lr: 1.2500e-05\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1726 - tversky_index: 0.8209 - jaccard_index: 0.7063 - recall: 0.8146 - precision: 0.8413 - specificity: 0.9631 - npv: 0.9552 - dice_coefficient: 0.8274\n",
      "Epoch 00036: loss improved from 0.17354 to 0.17256, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e36-ji0.70630-l0.17256-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.1726 - tversky_index: 0.8209 - jaccard_index: 0.7063 - recall: 0.8146 - precision: 0.8413 - specificity: 0.9631 - npv: 0.9552 - dice_coefficient: 0.8274 - lr: 6.2500e-06\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1732 - tversky_index: 0.8205 - jaccard_index: 0.7051 - recall: 0.8144 - precision: 0.8401 - specificity: 0.9620 - npv: 0.9542 - dice_coefficient: 0.8268\n",
      "Epoch 00037: loss did not improve from 0.17256\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.1732 - tversky_index: 0.8205 - jaccard_index: 0.7051 - recall: 0.8144 - precision: 0.8401 - specificity: 0.9620 - npv: 0.9542 - dice_coefficient: 0.8268 - lr: 6.2500e-06\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1725 - tversky_index: 0.8234 - jaccard_index: 0.7060 - recall: 0.8196 - precision: 0.8362 - specificity: 0.9608 - npv: 0.9558 - dice_coefficient: 0.8275\n",
      "Epoch 00038: loss improved from 0.17256 to 0.17255, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e38-ji0.70598-l0.17255-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.1725 - tversky_index: 0.8234 - jaccard_index: 0.7060 - recall: 0.8196 - precision: 0.8362 - specificity: 0.9608 - npv: 0.9558 - dice_coefficient: 0.8275 - lr: 6.2500e-06\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1663 - tversky_index: 0.8282 - jaccard_index: 0.7151 - recall: 0.8230 - precision: 0.8454 - specificity: 0.9637 - npv: 0.9569 - dice_coefficient: 0.8337\n",
      "Epoch 00039: loss improved from 0.17255 to 0.16634, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e39-ji0.71512-l0.16634-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.1663 - tversky_index: 0.8282 - jaccard_index: 0.7151 - recall: 0.8230 - precision: 0.8454 - specificity: 0.9637 - npv: 0.9569 - dice_coefficient: 0.8337 - lr: 3.1250e-06\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1859 - tversky_index: 0.8052 - jaccard_index: 0.6876 - recall: 0.7967 - precision: 0.8330 - specificity: 0.9617 - npv: 0.9509 - dice_coefficient: 0.8141\n",
      "Epoch 00040: loss did not improve from 0.16634\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.1859 - tversky_index: 0.8052 - jaccard_index: 0.6876 - recall: 0.7967 - precision: 0.8330 - specificity: 0.9617 - npv: 0.9509 - dice_coefficient: 0.8141 - lr: 3.1250e-06\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1748 - tversky_index: 0.8184 - jaccard_index: 0.7032 - recall: 0.8119 - precision: 0.8396 - specificity: 0.9621 - npv: 0.9538 - dice_coefficient: 0.8252\n",
      "Epoch 00041: loss did not improve from 0.16634\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.1748 - tversky_index: 0.8184 - jaccard_index: 0.7032 - recall: 0.8119 - precision: 0.8396 - specificity: 0.9621 - npv: 0.9538 - dice_coefficient: 0.8252 - lr: 3.1250e-06\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1655 - tversky_index: 0.8296 - jaccard_index: 0.7172 - recall: 0.8248 - precision: 0.8452 - specificity: 0.9637 - npv: 0.9574 - dice_coefficient: 0.8345\n",
      "Epoch 00042: loss improved from 0.16634 to 0.16545, saving model to /home/root/saved_models/Segmentalist_1a3fb03d-e33c-4809-a0ee-610675b3a678/cp-e42-ji0.71716-l0.16545-vl0.00.ckpt\n",
      "11/11 [==============================] - 30s 3s/step - loss: 0.1655 - tversky_index: 0.8296 - jaccard_index: 0.7172 - recall: 0.8248 - precision: 0.8452 - specificity: 0.9637 - npv: 0.9574 - dice_coefficient: 0.8345 - lr: 1.5625e-06\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1757 - tversky_index: 0.8168 - jaccard_index: 0.7018 - recall: 0.8097 - precision: 0.8405 - specificity: 0.9625 - npv: 0.9533 - dice_coefficient: 0.8243\n",
      "Epoch 00043: loss did not improve from 0.16545\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.1757 - tversky_index: 0.8168 - jaccard_index: 0.7018 - recall: 0.8097 - precision: 0.8405 - specificity: 0.9625 - npv: 0.9533 - dice_coefficient: 0.8243 - lr: 1.5625e-06\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1773 - tversky_index: 0.8162 - jaccard_index: 0.6993 - recall: 0.8100 - precision: 0.8365 - specificity: 0.9616 - npv: 0.9538 - dice_coefficient: 0.8227\n",
      "Epoch 00044: loss did not improve from 0.16545\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.1773 - tversky_index: 0.8162 - jaccard_index: 0.6993 - recall: 0.8100 - precision: 0.8365 - specificity: 0.9616 - npv: 0.9538 - dice_coefficient: 0.8227 - lr: 1.5625e-06\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1709 - tversky_index: 0.8230 - jaccard_index: 0.7085 - recall: 0.8172 - precision: 0.8419 - specificity: 0.9627 - npv: 0.9554 - dice_coefficient: 0.8291\n",
      "Epoch 00045: loss did not improve from 0.16545\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.1709 - tversky_index: 0.8230 - jaccard_index: 0.7085 - recall: 0.8172 - precision: 0.8419 - specificity: 0.9627 - npv: 0.9554 - dice_coefficient: 0.8291 - lr: 1.0000e-06\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1705 - tversky_index: 0.8248 - jaccard_index: 0.7092 - recall: 0.8202 - precision: 0.8397 - specificity: 0.9618 - npv: 0.9559 - dice_coefficient: 0.8295\n",
      "Epoch 00046: loss did not improve from 0.16545\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.1705 - tversky_index: 0.8248 - jaccard_index: 0.7092 - recall: 0.8202 - precision: 0.8397 - specificity: 0.9618 - npv: 0.9559 - dice_coefficient: 0.8295 - lr: 1.0000e-06\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1809 - tversky_index: 0.8149 - jaccard_index: 0.6950 - recall: 0.8112 - precision: 0.8285 - specificity: 0.9600 - npv: 0.9541 - dice_coefficient: 0.8191\n",
      "Epoch 00047: loss did not improve from 0.16545\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.1809 - tversky_index: 0.8149 - jaccard_index: 0.6950 - recall: 0.8112 - precision: 0.8285 - specificity: 0.9600 - npv: 0.9541 - dice_coefficient: 0.8191 - lr: 1.0000e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1722 - tversky_index: 0.8219 - jaccard_index: 0.7065 - recall: 0.8162 - precision: 0.8403 - specificity: 0.9620 - npv: 0.9549 - dice_coefficient: 0.8278\n",
      "Epoch 00048: loss did not improve from 0.16545\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.1722 - tversky_index: 0.8219 - jaccard_index: 0.7065 - recall: 0.8162 - precision: 0.8403 - specificity: 0.9620 - npv: 0.9549 - dice_coefficient: 0.8278 - lr: 1.0000e-06\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.1767 - tversky_index: 0.8210 - jaccard_index: 0.7000 - recall: 0.8189 - precision: 0.8282 - specificity: 0.9598 - npv: 0.9568 - dice_coefficient: 0.8233\n",
      "Epoch 00049: loss did not improve from 0.16545\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.1767 - tversky_index: 0.8210 - jaccard_index: 0.7000 - recall: 0.8189 - precision: 0.8282 - specificity: 0.9598 - npv: 0.9568 - dice_coefficient: 0.8233 - lr: 1.0000e-06\n"
     ]
    }
   ],
   "source": [
    "log = logging.getLogger()\n",
    "if True:\n",
    "    # \n",
    "    assert tf.test.is_gpu_available(), \"CHECK GPU AVAILABILITY! (eg /etc/docker/daemon.json default runtime)\"\n",
    "    #\n",
    "    np.random.seed(cfg.seed)\n",
    "    # set window/patch size\n",
    "    patch_dims = (256, 256)\n",
    "    # process necessary parser arguments\n",
    "    layer_blocks_ = [int(n) for n in '2,2,2,2'.split(',')]\n",
    "    residual_filters_ = [int(n) for n in '128,256,512,1024'.split(',')]\n",
    "    initial_kernel_size_ = (7, 7)\n",
    "    head_kernel_size_ = (1, 1)\n",
    "    # decide partitioning into train/validation\n",
    "    train_val_test_split = (1.-0.1, 0.1)\n",
    "    # --- assemble training datasets\n",
    "    # get dataset tags - sort to fix order to identify different permutations for array caching\n",
    "    dataset_tags = sorted(['train_tif'])\n",
    "    # get each of the training datasets requested\n",
    "    tdsets = []\n",
    "    for ds_tag in ['train_tif']:\n",
    "        ds = datasets.get_dataset(ds_tag)\n",
    "        rf = ds.spatial_resolution/0.4\n",
    "        tdsets.append(ds.load_training_data(batch_size=4, train_val_test_split=train_val_test_split, seed=cfg.seed, window_size=256, overlap_tiles=True, resample_factor=rf))\n",
    "    # combine them into one big (composite) training dataset\n",
    "    if len(tdsets) == 1:\n",
    "        tds = tdsets[0]\n",
    "    else:\n",
    "        tds = reduce(operator.add, tdsets)\n",
    "    # create a string to identify the combination of datasets and the spatial resolution\n",
    "    # used in saving model checkpoints to quickly identify training data used\n",
    "    data_res_str = f\"data_{tds.tags_str}_target_res_{0.4}\"\n",
    "    data_res_str += '_overlapping_tiles'\n",
    "    # set the cache directory to save preprocessed arrays in an appropriately named directory\n",
    "    tds.cache_directory = cfg.proc_data_path / Path(data_res_str)\n",
    "    # --- preprocess training data\n",
    "    # generate arrays from rasters on-the-fly at training time\n",
    "    tds.prepare()\n",
    "    # skipped              \n",
    "    # --- assign data generator for scaling, augmentations etc\n",
    "    # albumentations + fancyPCA => \"fancy\" augs. recommended.\n",
    "    if True:\n",
    "        log.warning(\"~~ Fancy augs on ~~\")\n",
    "        # start distributed cluster for mapping augmentations\n",
    "        client = Client(processes=False)\n",
    "        log.info(\"Calculating PCA decomposition of training RGBs...\")\n",
    "        fpca = FancyPCA(tds.X_train, alpha_std=.3, p=1.0)\n",
    "        log.warning(f\"Eigenvalues are {fpca.sampler.eig_vals}\")\n",
    "        log.warning(f\"Eigenvectors are {fpca.sampler.eig_vecs}\")\n",
    "        augger = strong_aug(p=.8, fancy_pca=fpca)\n",
    "        tds.batch_generator_fn = partial(fancy_batch_generator, batch_size=4, augger=augger, client=client, seed=cfg.seed, shuffle_blocks_every_epoch=True, shuffle_within_blocks=True, deep_supervision=True, float32=True)\n",
    "        aug_sfx = 'fancy'\n",
    "    ## --- configure model training\n",
    "    # get loss function and any kwargs (entered as a string to argparser)\n",
    "    loss_fn_name, *lf_kwarg_str = 'dice_coeff_loss'.split(':')\n",
    "    # grab func itself from losses module by name\n",
    "    loss_fn = getattr(losses, loss_fn_name)\n",
    "    # optionally provide kwargs to higher-order function to return lf\n",
    "    if lf_kwarg_str:\n",
    "        lf_kwargs = parse_kwarg_str(*lf_kwarg_str)\n",
    "        loss_fn = loss_fn(**lf_kwargs)\n",
    "    else:\n",
    "        lf_kwargs = {}\n",
    "    # encode loss function args as a cleaned-up string for identifying models trained with this\n",
    "    lfastr = '_args_' + '_'.join([f'{k}={v:.2f}' for k, v in lf_kwargs.items()]) if lf_kwargs else ''\n",
    "    # calculate number of training and validation steps\n",
    "    train_steps = tds.X_train.shape[0]//4\n",
    "    valid_steps = tds.X_val.shape[0]//4\n",
    "    print(f'train_steps={train_steps}')\n",
    "    print(f'val_steps={valid_steps}')\n",
    "    if True:\n",
    "        assert tds.X_val.shape[0] > 0, (\n",
    "            \"Not enough dask blocks to make up validation data frac!\\n\"\n",
    "            f\"train: {tds.X_train}\"\n",
    "        )\n",
    "    # select metrics\n",
    "    metrics = [losses.tversky_index, losses.jaccard_index, losses.recall, losses.precision, losses.specificity, losses.npv, losses.dice_coefficient]\n",
    "    # interpret optimizer\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False) # check out RADAM?\n",
    "    # stoachastic weight averaging if enabled\n",
    "    opt = SWA(opt, start_averaging=100 - 50, average_period=5)\n",
    "    # specify training directory to save weights and metrics for this loss_fn and data ID\n",
    "    project_name = Path(f'Segmentalist_{uuid.uuid4()}')\n",
    "    training_dir = Path(str(cfg.models_path)) / project_name\n",
    "    training_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # -- callbacks\n",
    "    # early stopping\n",
    "    monitor = 'loss'\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor, patience=7)]\n",
    "    # reduce the learning rate on plateaus\n",
    "    callbacks.append(tf.keras.callbacks.ReduceLROnPlateau(monitor=monitor,factor=0.5,patience=2, min_lr=0.000001))\n",
    "    # set up tensorboard to record metrics in a subdirectory\n",
    "    tb_pth = training_dir / Path(\"metrics/\")\n",
    "    tb_cb = tf.keras.callbacks.TensorBoard(log_dir=str(tb_pth),update_freq=50)\n",
    "    callbacks.append(tb_cb)\n",
    "    # set up checkpoints in the training directory\n",
    "    cp_fmt = 'cp-e{epoch:02d}-ji{jaccard_index:.5f}-l{loss:.5f}'\n",
    "    suffix = '-vl{val_loss:.5f}.ckpt'\n",
    "    cp_fmt = cp_fmt + '-vl0.00.ckpt' # suffix\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=str(training_dir / Path(cp_fmt)), monitor=monitor, save_best_only=True, save_weights_only=True, verbose=1)\n",
    "    cp_callback_trn = tf.keras.callbacks.ModelCheckpoint(filepath=str(training_dir / Path(cp_fmt)), monitor='loss', save_best_only=True, save_weights_only=True, verbose=1)\n",
    "    # callbacks.append(cp_callback)\n",
    "    callbacks.append(cp_callback_trn)\n",
    "    # --- build and compile the model\n",
    "    # format attention gate param\n",
    "    ag = 'SAG'\n",
    "    model = Segmentalist(n_classes=tds.y_train.shape[-1], layer_blocks=[2,2,2,2], last_decoder_layer_blocks=2, initial_filters=64, residual_filters=[128,256,512,1024],\n",
    "        initial_kernel_size=(7, 7), head_kernel_size=(1, 1), cardinality=1, act='relu', downsample='pool',\n",
    "        decoder_attention_gates=ag, encoder_cbam=True, decoder_cbam=True, pyramid_pooling=True, deep_supervision=True, lambda_conv=True,)\n",
    "    model.build(input_shape=(4, 256, 256, tds.X_train.shape[-1]))\n",
    "    model.compile(optimizer=opt, loss=loss_fn, metrics=metrics)\n",
    "    # option to dump test data removed for testing session\n",
    "    # dump setup\n",
    "    # augmentations\n",
    "    A.save(augger, f'{training_dir}/transform.yml', data_format='yaml')\n",
    "    # \n",
    "    log.info(\"Start training...\")\n",
    "    # \n",
    "    args = {'datasets': 'train_tif', 'target_spatial_resolution': 0.4, 'pyramid_pooling': True, 'deep_supervision': True, 'lambda_conv': True, 'encoder_cbam': True, 'decoder_cbam': True, 'sag': True, 'csag': True, 'layer_blocks': '2,2,2,2', 'layer_blocks_' : [2,2,2,2], 'last_decoder_layer_blocks': 2, 'initial_filters': 64, 'residual_filters': '128,256,512,1024', 'residual_filters_' : [128,256,512,1024], 'initial_kernel_size': 7, 'initial_kernel_size_' : (7, 7), 'head_kernel_size': 1, 'head_kernel_size_' : (1, 1), 'cardinality': 1, 'activation': 'relu', 'downsample': 'pool', 'patch_size': 256, 'overlapping_tiles': True, 'epochs': 100, 'batch_size': 4, 'loss_fn': 'dice_coeff_loss', 'optimiser': 'adam', 'swa': True, 'duration_swa': 50, 'period_swa': 5, 'use_val': False, 'patience': 7, 'seed': 42, 'val_frac': 0.1, 'test_frac': 0.0, 'fancy_augs': True, 'lr_init': 0.0001, 'lr_min': 1e-06, 'lr_reduce_factor': 0.5, 'lr_reduce_patience': 2, 'ocp': False, 'balanced_oversample': False, 'models_dir': str(cfg.models_path), 'dump_test_data': False, 'dump_first_batches': False, 'use_cache': False, 'save_to_cache': False}\n",
    "    # \n",
    "    with open(f'{training_dir}/run_params.yml', 'w') as outfile:\n",
    "        yaml.dump(args, outfile, default_flow_style=False)\n",
    "    # --- train the model\n",
    "    print('fit model')\n",
    "    model.fit(tds.batch_gen_train(), steps_per_epoch=train_steps, epochs=100, max_queue_size=50, callbacks=callbacks)\n",
    "    # validation_data=tds.batch_gen_val(), validation_steps=valid_steps, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uuid4                                     1a3fb03d-e33c-4809-a0ee-610675b3a678\n",
       "training_dir                 ../../saved_models/Segmentalist_1a3fb03d-e33c-...\n",
       "lowest_loss_ckpt             ../../saved_models/Segmentalist_1a3fb03d-e33c-...\n",
       "lowest_val_loss_ckpt         ../../saved_models/Segmentalist_1a3fb03d-e33c-...\n",
       "lowest_loss                                                           0.165450\n",
       "lowest_val_loss                                                       0.000000\n",
       "X0_val                                                                    None\n",
       "y0_val                                                                    None\n",
       "activation                                                                relu\n",
       "balanced_oversample                                                      False\n",
       "batch_size                                                                   4\n",
       "cardinality                                                                  1\n",
       "csag                                                                      True\n",
       "datasets                                                              test_tif\n",
       "decoder_cbam                                                              True\n",
       "deep_supervision                                                          True\n",
       "downsample                                                                pool\n",
       "duration_swa                                                                50\n",
       "encoder_cbam                                                              True\n",
       "epochs                                                                     100\n",
       "head_kernel_size                                                             1\n",
       "head_kernel_size_                                                       (1, 1)\n",
       "initial_filters                                                             64\n",
       "initial_kernel_size                                                          7\n",
       "initial_kernel_size_                                                    (7, 7)\n",
       "lambda_conv                                                               True\n",
       "last_decoder_layer_blocks                                                    2\n",
       "layer_blocks                                                           2,2,2,2\n",
       "layer_blocks_                                                     [2, 2, 2, 2]\n",
       "loss_fn                                                        dice_coeff_loss\n",
       "lr_init                                                               0.000100\n",
       "lr_min                                                                0.000001\n",
       "lr_reduce_factor                                                      0.500000\n",
       "lr_reduce_patience                                                           2\n",
       "models_dir                                             /home/root/saved_models\n",
       "optimiser                                                                 adam\n",
       "overlapping_tiles                                                         True\n",
       "patch_size                                                                 256\n",
       "patience                                                                     7\n",
       "period_swa                                                                   5\n",
       "pyramid_pooling                                                           True\n",
       "residual_filters                                              128,256,512,1024\n",
       "residual_filters_                                        [128, 256, 512, 1024]\n",
       "sag                                                                       True\n",
       "swa                                                                       True\n",
       "target_spatial_resolution                                             0.400000\n",
       "test_frac                                                             0.000000\n",
       "use_cache                                                                False\n",
       "use_val                                                                  False\n",
       "val_frac                                                              0.100000\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_segm_trained = utils.collate_run_data(Path(str(cfg.models_path)), model_name='Segmentalist')\n",
    "# \n",
    "df_segm_trained = df_segm_trained.drop(['seed', 'save_to_cache'] + [c for c in df_segm_trained.columns if any(i in c for i in ('_train',  'ocp', 'fancy_augs', 'dump_test_data', 'dump_first_batches', 'channel_expansion_factor'))], axis=1).sort_values(by='lowest_loss')\n",
    "# we can select a sub\n",
    "view_cols = ['datasets', 'lowest_loss', 'balanced_oversample', 'batch_size', 'patch_size', 'overlapping_tiles', 'deep_supervision', 'target_spatial_resolution', 'pyramid_pooling', 'loss_fn', 'val_frac']\n",
    "# \n",
    "df_segm_trained[view_cols + ['uuid4'] ] # \n",
    "row = df_segm_trained.query('datasets == \"train_tif\"').iloc[0]\n",
    "# row\n",
    "inference_window_size = 1024\n",
    "# load again with new patch shape\n",
    "model = Segmentalist.load_from_metadata(row=row)\n",
    "# shouldn't have to run these lines, LRM will look at the method above and fix so weight loading is included properly...\n",
    "# workaround which fixes weight loading; \"prime\" model on something of the right shape\n",
    "model(np.random.rand(1, inference_window_size, inference_window_size, 3))\n",
    "model.load_weights(row.lowest_loss_ckpt) # takes a wee minute\n",
    "model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(arr, patch_shape=(640,640,3), extraction_step=512):\n",
    "    arr_ndim = arr.ndim\n",
    "\n",
    "    if isinstance(patch_shape, numbers.Number):\n",
    "        patch_shape = tuple([patch_shape] * arr_ndim)\n",
    "    if isinstance(extraction_step, numbers.Number):\n",
    "        extraction_step = tuple([extraction_step] * arr_ndim)\n",
    "\n",
    "    patch_strides = arr.strides\n",
    "\n",
    "    slices = tuple(slice(None, None, st) for st in extraction_step)\n",
    "    indexing_strides = arr[slices].strides\n",
    "\n",
    "    patch_indices_shape = ((np.array(arr.shape) - np.array(patch_shape)) //\n",
    "                           np.array(extraction_step)) + 1\n",
    "\n",
    "    shape = tuple(list(patch_indices_shape) + list(patch_shape))\n",
    "    strides = tuple(list(indexing_strides) + list(patch_strides))\n",
    "\n",
    "    patches = as_strided(arr, shape=shape, strides=strides)\n",
    "    return patches\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dir = str(cfg.infer_data_tif_path) # '/home/root/tests/resources/infer_data_tif' # infer_data_tif\n",
    "tif_files = [tif for tif in os.listdir(path_to_dir) if '.tif' in tif]\n",
    "for image_name in tif_files:\n",
    "    print('-----------------------------------------------------------')\n",
    "    print(\"Running prediction on image:  \", image_name)\n",
    "    # specify the repository where the input file is located and its name\n",
    "    # path_to_dir = '/home/root/data/volumes/datasets/baranquilla_full_40cm/' # baranquilla_merged_33_resampled_40cm\n",
    "    path_to_file_complete = os.path.join(path_to_dir, image_name)\n",
    "    # print(\"----------------------------------------------------------------------------------------------------\")\n",
    "    # print(path_to_file_complete)\n",
    "    img = imageio.imread(path_to_file_complete)\n",
    "    # plt.imshow(img)\n",
    "    # patches = extract_patches(img)\n",
    "    # print(patches.shape)\n",
    "    # print(patches.reshape((-1, 1024, 1024, 3)).shape)\n",
    "    image_size = img.shape\n",
    "    # print(\"image size :\", image_size)\n",
    "    input_patch_size = (512, 512, 3)\n",
    "    padded_image_size = [math.ceil(image_size[i]/input_patch_size[i]) * (input_patch_size[i]) for i in range(3)]\n",
    "    # print(\"padded image size :\", padded_image_size)\n",
    "    topad = []\n",
    "    for i in range(len(image_size)-1):\n",
    "        if (padded_image_size[i] - image_size[i])%2 == 0:\n",
    "            topad.append((int((padded_image_size[i]- image_size[i])/2), int((padded_image_size[i]- image_size[i])/2)))\n",
    "        else:\n",
    "            topad.append((int((padded_image_size[i]- image_size[i])//2), int((padded_image_size[i]- image_size[i])//2)+1))\n",
    "    topad.append((0,0))\n",
    "    # print(tuple(topad))\n",
    "    ###\n",
    "    ret = np.pad(img, pad_width=tuple(topad), mode='reflect')\n",
    "    # print(\"Confirm new padded image size :\", ret.shape)\n",
    "    ###\n",
    "    # proceed to extract overlapping patches\n",
    "    ret_patches = extract_patches(ret, patch_shape=(512,512,3), extraction_step=512)\n",
    "    # print(ret_patches.shape)\n",
    "    # X = np.transpose(patches.reshape((-1, 32, 32, 3)), (0, 3, 1, 2))\n",
    "    # print(ret_patches.reshape((-1, 512, 512, 3)).shape)\n",
    "    # plt.imshow(ret)\n",
    "    # Extra padding step to account for patch overlap during prediction process\n",
    "    aug_img = np.pad(ret, pad_width=((256, 256), (256, 256), (0, 0)), mode='reflect')\n",
    "    # print(\"Check newly padded (augmented) image size :\", aug_img.shape)\n",
    "    # plot augmented image\n",
    "    # plt.imshow(aug_img)\n",
    "    # proceed to extract overlapping patches\n",
    "    aug_patches = extract_patches(aug_img, patch_shape=(1024,1024,3), extraction_step=512)\n",
    "    topredict = aug_patches.reshape((-1, 1024, 1024, 3))\n",
    "    pred_aug_patches = []\n",
    "    for i in range(aug_patches.reshape((-1, 1024, 1024, 3)).shape[0]):\n",
    "        pred_aug_patches.append(model(topredict[i,:,:,:][None, ...]/255.))\n",
    "    pred_aug_patchex = np.array(np.squeeze(pred_aug_patches))\n",
    "    (I,J,K) =  (ret.shape[0]//512, ret.shape[1]//512, 3)\n",
    "    reconst_img = np.empty((ret.shape[0],ret.shape[1]), float)\n",
    "    index = 0\n",
    "    for i in range(I):\n",
    "        # print('j : ', j)\n",
    "        for j in range(J):\n",
    "            # print('i : ', i)\n",
    "            reconst_img[512*i:512*(i+1), 512*j:512*(j+1)] = pred_aug_patchex[index,256:1024-256,256:1024-256] # aug_patches.reshape((-1, 640, 640, 3))[index,64:640-64,64:640-64,:]\n",
    "            index += 1\n",
    "        #\n",
    "    #\n",
    "    # print()\n",
    "    # print(reconst_img.shape)\n",
    "    plt.imshow(reconst_img)\n",
    "    reconst_img_origin = reconst_img[topad[0][0]:reconst_img.shape[0]-topad[0][1],topad[1][0]:reconst_img.shape[1]-topad[1][1]]\n",
    "    ###\n",
    "    # np.save('predImg_output_xyz.npy', reconst_img_origin)\n",
    "    ###\n",
    "    datenow = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    output_name = \"Model_Segmentalist_infer_\" + image_name + \"_\" + datenow + \".tif\" # 'predictedImgOutput.tif'\n",
    "    output_path = cfg.predictions_data_tif_path / Path(output_name)\n",
    "    with rasterio.open(path_to_file_complete) as src:\n",
    "        profile = src.profile\n",
    "        profile.update(count=1)\n",
    "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "            #\n",
    "            dst.write(tf.cast(reconst_img_origin*255., tf.uint8), indexes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

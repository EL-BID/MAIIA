

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Models &mdash; MAIIA 0.0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Training" href="training.html" />
    <link rel="prev" title="Preprocessing" href="preprocessing.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> MAIIA
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Segmentation models:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="datasets.html">Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="preprocessing.html">Preprocessing</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#deepresunet">DeepResUNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optional-model-features">Optional Model Features</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#spatial-attention-gates">Spatial Attention Gates</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deep-supervision">Deep Supervision</a></li>
<li class="toctree-l3"><a class="reference internal" href="#input-pyramid-pooling">Input Pyramid Pooling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#grouped-convolutions">Grouped Convolutions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#convolutional-block-attention-module-cbam">Convolutional Block Attention Module (CBAM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#channel-spatial-attention-gates">Channel-Spatial Attention Gates</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#segmentalist">Segmentalist</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="training.html">Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="polygonisation.html">Polygonisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="misc.html">Miscellaneous</a></li>
</ul>
<p class="caption"><span class="caption-text">Main package:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="gim_cv.html">gim_cv package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MAIIA</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Models</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/models.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="models">
<h1>Models<a class="headerlink" href="#models" title="Permalink to this headline">¶</a></h1>
<p>The current model of choice is <code class="docutils literal notranslate"><span class="pre">Segmentalist</span></code> which builds on the <a class="reference external" href="https://www.mdpi.com/2072-4292/11/15/1774/htm">DeepResUNet</a> architecture
by adding various optional additional architectural components.
The following sections will give an overview of each of these. Using this model with
any or all these optional components will be covered in the section <a class="reference internal" href="#segmentalist"><span class="std std-ref">Segmentalist</span></a>.</p>
<p>For completeness, the relevant schematics from various publications will be reproduced
here to make this page a quick reference. For full information, the included links to the source
articles should be consulted.</p>
<section id="deepresunet">
<h2>DeepResUNet<a class="headerlink" href="#deepresunet" title="Permalink to this headline">¶</a></h2>
<p>The DeepResUNet architecture is described in the paper <a class="reference external" href="https://www.mdpi.com/2072-4292/11/15/1774/htm">Semantic Segmentation of Urban Buildings from VHR Remote Sensing Imagery Using a Deep Convolutional Neural Network (Yi et al. 2019)</a>.</p>
<p>This is a fully convolutional encoder-decoder segmentation network with residual convolution blocks
(as in ResNet) and skip connections (as in U-Net).</p>
<figure class="align-default" id="id3">
<img alt="The DeepResUNet architecture block" src="_images/drunet.png" />
<figcaption>
<p><span class="caption-text">The DeepResUNet architecture.</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>It should be noted that from experiments, the pattern chosen in this paper for fixing the filter size
and number of channels within each residual block leads to a significant performance increase over
the vanilla ResNet residual block for segmentation tasks.</p>
<figure class="align-default" id="id4">
<img alt="The DeepResUNet architecture" src="_images/drunet_resblock.png" />
<figcaption>
<p><span class="caption-text">The residual block structure used in DeepResUNet (d) as compared with ResNet (c).</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>This network architecture forms the backbone of <a class="reference internal" href="#segmentalist"><span class="std std-ref">Segmentalist</span></a>, i.e. is equivalent to it with
the following additional features disabled.</p>
</section>
<section id="optional-model-features">
<h2>Optional Model Features<a class="headerlink" href="#optional-model-features" title="Permalink to this headline">¶</a></h2>
<section id="spatial-attention-gates">
<h3>Spatial Attention Gates<a class="headerlink" href="#spatial-attention-gates" title="Permalink to this headline">¶</a></h3>
<p>The Spatial Attention Gate modules implemented are described in the paper
<a class="reference external" href="http://arxiv.org/abs/1804.03999">Attention U-Net: Learning Where to Look for the Pancreas (Oktay et al. 2018)</a>.</p>
<p>Spatial attention gates provide a mechanism through which the important regions
of the encoder feature maps can be enhanced or suppressed depending on the more
abstract semantic content of the decoder at the corresponding spatial locations.</p>
<p>These use use kernel-size one convolutions to project each
set of encoder feature maps into a new space of “key” vectors at each spatial
location, and decoder feature maps into a new space of “query” vectors at each
spatial location. These feature maps are then used to derive an (additive)
attention map by adding these and applying a further 1D convolution with sigmoid
activation. This is then multiplied with the encoder feature maps, adaptively
rescaling them according to the content of the decoder feature maps.</p>
<figure class="align-default" id="id5">
<img alt="The Spatial Attention Gate module" src="_images/sag.png" />
<figcaption>
<p><span class="caption-text">The Spatial Attention Gate module. <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">g</span></code> are feature maps coming through
the encoder skip lines and from the deeper decoder stages respectively. These
are mapped into a key, query space with kernel-size 1 convolutions, added and
the a ReLU nonlinearity applied to the result. A final convolution with sigmoid
activation is used to generate a spatial attention map which then reweights <code class="docutils literal notranslate"><span class="pre">x</span></code>.</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The spatial attention gates are inserted immediately after each decoder upsampling
block and intercept the encoder feature maps through the skip lines before these
enter the next decoder block.</p>
<figure class="align-default" id="id6">
<img alt="The Spatial Attention Gate architecture" src="_images/sag_architecture.png" />
<figcaption>
<p><span class="caption-text">A U-Net with spatial attention gates positioned before each intermediate decoder
block.</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>These have been demonstrated to improve encoder-decoder type models in various
segmentation tasks.</p>
</section>
<section id="deep-supervision">
<h3>Deep Supervision<a class="headerlink" href="#deep-supervision" title="Permalink to this headline">¶</a></h3>
<p>The version of deep supervision implemented follows the version described in the paper
<a class="reference external" href="https://www.frontiersin.org/articles/10.3389/frobt.2020.00106/full">Improving CT Image Tumor Segmentation Through Deep Supervision and Attentional Gates (Tureckova et al. 2020)</a>.</p>
<p>Deep supervision is a mechanism to force each decoder block of the network to take on
a more concrete role, namely learning to produce outputs which more directly correspond
to the target segmentation map at that block’s spatial resolution. It is known to improve
results and speed up training in segmentation tasks and is used heavily in biomedical imaging.</p>
<figure class="align-default" id="id7">
<img alt="The Deep Supervision network" src="_images/ds.jpg" />
<figcaption>
<p><span class="caption-text">A Deep Supervision network - each decoder output feature map is projected into the same
space used to generate the final segmentation map and these are added (with upsampling).</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The version implemented captures the output of the intermediate decoder feature maps and
projects these into the same (channel) space as the final output feature maps used to
generate the segmentation (before application of softmax/sigmoid). The segmentation map
produced is calculated by applying the final activation to the sum of all of these intermediate
decoder feature maps upsampled to the same resolution as the final decoder feature map, so that
each decoder block learns to make a direct contribution to the output class probabilities.</p>
<p>Note that other versions of deep supervision directly generate multiple output segmentation maps
(at different spatial resolutions) and train directly on the ground truth mask resampled to
match these.</p>
</section>
<section id="input-pyramid-pooling">
<h3>Input Pyramid Pooling<a class="headerlink" href="#input-pyramid-pooling" title="Permalink to this headline">¶</a></h3>
<p>Input pyramid pooling is implemented as in the paper
<a class="reference external" href="http://arxiv.org/abs/1810.07842">A Novel Focal Tversky loss function with improved Attention U-Net for lesion segmentation (Abraham et al. 2019)</a>.</p>
<p>Pyramid pooling can have slightly different meanings depending on the context. The version
implemented here makes the input image available to the network encoder at multiple spatial resolutions
through downsampling. This way each stage of the network encoder has access to both the
feature maps of the previous encoder layer (which access the original image indirectly)
and to the image itself directly, resampled to the spatial resolution of each encoder block.</p>
<p>Each intermediate encoder block is preceded by an additional set of convolutional filters
which generate feature maps from the coarsened input image. These coarse feature
maps are concatenated with the output of the previous encoder block and these together
form the inputs of the encoder block.</p>
<figure class="align-default" id="id8">
<img alt="Network architecture with ds + pp + sag" src="_images/ds_pp_sag.png" />
<figcaption>
<p><span class="caption-text">Network architecture with input pyramid pooling, spatial attention gates and (another
variant of) deep supervision.</span><a class="headerlink" href="#id8" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Empirically the improvements from pyramid pooling seem to be marginal with respect to most of the
other architectural enhancements described, and the additional feature maps do make networks
with these enabled heavier.</p>
</section>
<section id="grouped-convolutions">
<h3>Grouped Convolutions<a class="headerlink" href="#grouped-convolutions" title="Permalink to this headline">¶</a></h3>
<p>Grouped convolutions are implemented according to the “ResNeXt” block described in the paper
<a class="reference external" href="http://arxiv.org/abs/1611.05431">Aggregated Residual Transformations for Deep Neural Networks (Xie et al. 2016)</a>.</p>
<figure class="align-default" id="id9">
<img alt="grouped convs" src="_images/grouped_convs.png" />
<figcaption>
<p><span class="caption-text">Residual grouped convolution block (right) compared to regular residual block (left)</span><a class="headerlink" href="#id9" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>These are variations on a standard ResNet block which derive a set of <em>cardinality</em> low-dimensional
projections of feature maps and apply 3x3 convolutions independently to these before re-concatenating
them. While these worked well in image classification tasks, we’ve found empirically that they don’t
improve performance in segmentation.</p>
</section>
<section id="convolutional-block-attention-module-cbam">
<h3>Convolutional Block Attention Module (CBAM)<a class="headerlink" href="#convolutional-block-attention-module-cbam" title="Permalink to this headline">¶</a></h3>
<p>CBAM is implemented according to the paper <a class="reference external" href="http://arxiv.org/abs/1807.06521">CBAM: Convolutional Block Attention Module (Woo et al. 2018)</a>.</p>
<p>The CBAM module is in essence a simplified form of self-attention which enables a set of feature maps (from the
output of a residual block) to calculate channel and spatial attention maps for reweighting themselves.</p>
<figure class="align-default" id="id10">
<img alt="CBAM mod" src="_images/cbam_module.png" />
<figcaption>
<p><span class="caption-text">The channel and spatial attention modules making up the CBAM. The channel module passes a vector of
the max and average values of each channel across the whole spatial domain through an MLP and learns
a channel-wise reweighting with a sigmoid activation function. The spatial attention module performs
a global max and average pooling operation to derive two channel feature descriptors across the spatial
extent of the feature maps which are passed through a sigmoid convolution to derive a spatial attention
map. The kernel size of the final convolution is a hyperparameter which provides a degree of
context-awareness in the derivation of the attention values.</span><a class="headerlink" href="#id10" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The channel and spatial attention maps reweight the feature maps sequentially, i.e. the input feature maps
are first used to calculate channel attention maps, the channels are reweighted, and these reweighted
feature maps are used to calculate spatial attention which reweights the whole set (spatially) one more time.</p>
<figure class="align-default" id="id11">
<img alt="cbam_block" src="_images/cbam_block.png" />
<figcaption>
<p><span class="caption-text">The CBAM block as positioned in a residual block; i.e. as a feature map postprocessing step.</span><a class="headerlink" href="#id11" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>These blocks are quite lightweight (due to the channel and spatial pooling operations) and the dynamic
reweighting capability has been shown to lead to performance gains in various tasks. In experiments so
far these yield a minor net benefit to segmentation performance.</p>
</section>
<section id="channel-spatial-attention-gates">
<h3>Channel-Spatial Attention Gates<a class="headerlink" href="#channel-spatial-attention-gates" title="Permalink to this headline">¶</a></h3>
<p>TODO: doesn’t work as well as spatial attention in its current incarnation.</p>
</section>
</section>
<section id="segmentalist">
<span id="id1"></span><h2>Segmentalist<a class="headerlink" href="#segmentalist" title="Permalink to this headline">¶</a></h2>
<p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">Segmentalist</span></code> model is a configurable
fully convolutional residual encoder-decoder neural network with several optional
features. It can be created as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gim_cv.models.segmentalist</span> <span class="kn">import</span> <span class="n">Segmentalist</span>

<span class="c1"># basic configuration with optional features off</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Segmentalist</span><span class="p">(</span>
    <span class="n">layer_blocks</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="c1"># number of residual blocks per intermediate encoder/decoder stage</span>
    <span class="n">last_decoder_layer_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="c1"># number of residual blocks in the final stage</span>
    <span class="n">initial_filters</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="c1"># number of convolutional filters in first layer</span>
    <span class="n">residual_filters</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">256</span><span class="p">,</span><span class="mi">512</span><span class="p">],</span> <span class="c1"># number  of residual filters per conv block per stage</span>
    <span class="n">initial_kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">),</span> <span class="c1"># convolution kernel size in the first layer</span>
    <span class="n">head_kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="c1"># final kernel size to produce segmentation map</span>
    <span class="n">cardinality</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># cardinality (ResNeXt grouped convolution parameter). 1 implies regular convs</span>
    <span class="n">act</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="c1"># activation function</span>
    <span class="n">downsample</span><span class="o">=</span><span class="s1">&#39;pool&#39;</span><span class="p">,</span> <span class="c1"># downsampling method, &#39;pool&#39; or &#39;strides&#39;</span>
    <span class="n">decoder_attention_gates</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="c1"># None, &#39;SAG&#39; (spatial attention gate) or &#39;CSAG&#39; (channel + &lt;-)</span>
    <span class="n">encoder_cbam</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="c1"># if True, enable CBAM module in encoder residual blocks</span>
    <span class="n">decoder_cbam</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="c1"># if True, enable CBAM module in decoder residual blocks</span>
    <span class="n">pyramid_pooling</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="c1"># if True, enable pyramid spatial pooling (multi-scale inputs)</span>
    <span class="n">deep_supervision</span><span class="o">=</span><span class="kc">False</span> <span class="c1"># if True, enable deep supervision</span>
<span class="p">)</span>
</pre></div>
</div>
<p>In its basic configuration (as instantiated above) it’s equivalent to a <a class="reference external" href="https://www.mdpi.com/2072-4292/11/15/1774/htm">DeepResUNet</a> model. Each of
the various architectural enhancements above can be switched on independently. These can either modify
the residual blocks themselves (in the case of grouped convolutions and the CBAM module), or they can
add stages to the architecture itself (attention gates in the decoder, input pyramid pooling to the
encoder block inputs, deep supervision to the decoder block outputs).</p>
<p>The <code class="xref py py-meth docutils literal notranslate"><span class="pre">load_from_metadata()</span></code> method allows one to construct
a model from a row of a pandas dataframe containing the appropriate architecture initialisation
parameters. See the API documentation for more details.</p>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="training.html" class="btn btn-neutral float-right" title="Training" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="preprocessing.html" class="btn btn-neutral float-left" title="Preprocessing" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>